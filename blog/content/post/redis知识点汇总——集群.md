---
title: redis知识点汇总——集群
date: 2023-01-08T18:06:57+08:00
draft: false
categories: ["redis"]
tags: ["redis"]
---

# redis知识点汇总——集群
#技术/数据库/redis

## 槽
1. redis集群通过分片的方式来保存数据库中的键值对：集群的整个数据库被分为16384个槽。数据库中的每一个键都属于这16384个槽中的一个。集群中的每个节点可以处理0～16384个槽
2. 当数据库中的16384个槽都有节点在处理，集群处于上线状态；如果有任何一个槽没有得到处理，集群处于下线状态
3. 集群使用公式 CRC16(key) % 16384 来计算每次请求的键 key 属于哪个槽，通过查询集群配置，便可知道 key 对应的槽属于哪个 redis 节点，然后再将请求打到该节点
4. 通过将哈希槽分布到不同节点，我们可以很容易地向集群中添加或者删除节点

## MOVED错误
当节点发现客户端请求的键并非由自己处理时，节点会向客户端返回一个MOVED错误，指引客户端转向至正在负责槽的节点
> 格式：MOVED slot ip:port

## 节点
集群节点只能使用0号数据库

## 重新分片
重新分片可以将任意数量已经指派给某个节点的槽改为指派给另一个节点。并且相关槽所属的键值对也会从源节点被移动到目标节点。重新分片可以在线进行，集群不需要下线，源节点和目标节点都可以继续处理命令请求

### 步骤
1. redis-trib工具在目标节点执行`cluster setslot $slot importing $源节点`，准备导入源节点的slot
2. redis-trib工具在源节点执行`cluster setslot $slot migrating $目标节点`，准备从源节点向目标节点迁出slot
3. redis-trib工具在源节点执行`cluster getkeysinslot $slot $count`，获得最多$count个$slot中的键
4. 在源节点执行`migrate $目标节点 $目标端口 $key 0 $timeout` 对每个键进行迁移
5. 不断迁移每个键，直到该slot的所有键都被迁移到了目标节点

### ASK错误
1. 重新分配期间，源节点向一个目标节点迁移一个槽时，被迁移槽的一部分键值对保存在源节点中，而另一部分键值对保存在目标节点中。
2. 当客户端向源节点发送一个键的命令，若这个键恰好属于正在被迁移的槽
3. 源节点先在自己的数据库里查找指定的键，如果找到了就直接执行客户端发送的指令
4. 如果没有找到该key，则可能这个键已经被迁移走了，给客户端返回一个ASK错误，指引客户端转向正在导入的目标节点
5. 客户端向转向的节点发送一个ASKING命令，再发送键的命令
6. 因为若不发送ASKING命令，此时迁移的槽还未迁移完，并未指派给目标节点，目标节点此时是不会处理这个key的，于是又返回MOVED将这个请求踢回源节点
7. 如果目标节点发现该键属于的槽正在被自己导入，且带有ASKING，因此会破例处理一次

## 复制与故障转移
### 主节点
主节点用于处理槽
### 从节点
从节点用于复制某个主节点
### 故障转移
1. 集群中的所有节点互相ping对方，如果某一主节点没有及时回复pong，那么这个主节点会被没ping通的节点标识为PFAIL，传播给其他节点
2. 如果半数以上的主节点都标识某个节点为PFAIL，那么收到这个半数以上结果的节点会将这个PFAIL节点标识为FAIL节点，广播到集群中
3. 如果这个被标识为FAIL的主节点的从节点发现了自己的主节点是FAIL状态，向集群广播一条投票请求，要求收到这个投票请求的主节点给自己投票
4. 如果一个主节点在线，且还未投票过，就会投票给这个从节点，对于每一个配置纪元，主节点只能投票一次
5. 如果一个从节点获得了半数以上的投票，就会被选举为新的主节点
6. 如果一个配置纪元里没有从节点能够收集到足够多的选票，那么集群进入一个新的配置纪元重新进行选举，直到选举出一个新的主节点
### 数据丢失
1. 异步复制导致
对于Redis主节点与从节点之间的数据复制，是异步复制的，当客户端发送写请求给master节点的时候，客户端会返回OK，然后同步到各个slave节点中。如果此时master还没来得及同步给slave节点时发生宕机，那么master内存中的数据会丢失
2. 网络分区
客户端和一些节点在一个网络分区，另一部分节点在另一个网络分区。在分区期间，客户端仍然能执行命令，直到集群经过cluster-node-timeout发现分区情况，节点探测到有slot无法提供服务，才开始禁止客户端执行命令。
这时候会出现一种现象，假设客户端和一个master在小分区，其他节点在大分区。超时后，其他节点共同投票把group内的一个slave提为master，等分区恢复。旧的master会成为新master的slave。这样在cluster-node-timeout期间对旧master的写入数据都会丢失

## 集群不可用的条件
1. 集群中的大部分主节点都进入都进入 PFAIL 状态时，集群也会进入 FAIL 状态
2. 一个主节点进入了FAIL状态且它没有从节点

## 集群的作用
1. 数据分区：集群将数据分散到多个节点，一方面突破了Redis单机内存大小的限制，存储容量大大增加；另一方面每个主节点都可以对外提供读服务和写服务，大大提高了集群的响应能力
2. 高可用：集群支持主从复制和主节点的自动故障转移

## 常见问题
1. 为什么必须3个主节点
- 一个节点，不能给自己投票。
- 两个节点 A 说 B 下线，B 认为 A 下线，两个人互相说我连接不上你，没有定论。
- 至少三个节点，A、B 发现 C 不通，互相通知，得到一致性状态：C 的确下线。

2. 为什么有16384个槽
	1. 如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。
如上所述，在消息头中，最占空间的是myslots[CLUSTER_SLOTS/8]，这是一个bitmap。 当槽位为65536时，这块的大小是: 65536÷8÷1024=8kb 因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽
	2. 集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者，不建议redis cluster节点数量超过1000个。 那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了
	3. myslots这个bitmap，如果节点很少，槽很多，很难被压缩
