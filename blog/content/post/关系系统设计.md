---
title: 关系系统设计
date: 2023-01-08T18:06:57+08:00
draft: false
categories: ["系统设计"]
tags: ["系统设计"]
---

# 关系系统设计
#技术/系统设计

## 题目

1. 获取用户的关注和粉丝列表； 
2. 获取用户关注和粉丝计数；
3. 给定多个用户，判断用户是否关注，是否为指定用户的粉丝，是否为双向关系；

数据量预估： 
1. 总关系数据量：50亿；写入QPS：1000qps；
2. 各类接口查询QPS：70w qps，其中check relation占50w；


1. 业务考察点：
	1. 给系统接口设计，及相应业务实现逻辑和存储数据结构设计方案；
	2. 如何解决粉丝数较多时，获取用户粉丝列表查询效率；（5000以下走缓存，5000以上提供离线服务计算）
	3. check relation如何保证数据准确性，特别是粉丝或关注数较多用户；

2. 工程考察点：
	1. 在高QPS和存储量下，之前设计的数据结构和业务会存在什么问题？（开放性，针对候选人设计提问）
	2. 根据给定数据量和业务请求QPS，评估系统存储和计算资源需求；
	3. 如何保证基础系统高可用性，需要给出不同场景下依赖故障的高可用解决方案（比如依赖存储故障，单机房故障等）



## 方案设计
### 存储
#### mysql
```
id //自增id
from_user_id //主动用户id
to_user_id //被动用户id
create_time 
modify_time
type //关系类型
bussiness_id //业务id
```
需要建立两个表，一个正向表，一个反向表；
正向表是用来存储a -> b的关注关系，即关注表；
对于查询a -> b的关注列表可以直接`select to_user_id from table where from_user_id = $from_user_id`
查询粉丝关系，即b的粉丝，可以使用`select from_user_id from table where to_user_id = $from_user_id`
但是由于mysql需要分片存储，分片如果按from_user_id来分的话，粉丝关系的查询就无法使用分片查询了，所以需要构建正向表和反向表，反向表（粉丝表）用于进行粉丝关系的判断和查询；

出于性能考虑，可以对反向表进行异步更新：消费正向表的binlog来更新反向表，反向表使用to_user_id进行分片；

#### redis
1. 关系列表缓存
key value存储，key为from_user_id，value为列表zlib压缩后进行存储
2. hash_cache
hash存储，key为from_user_id，field为to_user_id，value为0，1；存储a -> b的关系，用来判断a -> b是否存在关系。
0代表未关注，代表缓存有效；如果值为空（不为0/1，则说明缓存失效，需要回源）

缓存的细节问题：
1. 正向关系，先更新db，然后更新缓存，不删除缓存。因为正向关系通常是一个人自己发起的，很难出现并发的情况，因此不需要删除缓存来保持一致性；
2. 反向关系，更新完db后，直接删除缓存。因为反向关系，例如粉丝列表，很可能出现多个人同时取关/关注某一个大V，如果直接更新缓存可能会有并发问题（先后两个请求1、2，请求1：a关注c，请求2：b关注c，更新缓存时先更新了请求2，然后更新请求1，此时缓存里请求2会被覆盖掉）
3. 回源分布式锁，防止出现缓存失效，大量回源的场景：读哪个redis就在哪个redis上上锁，key为当前需要回源的redis的key+mute前缀，ttl为2s，lock失败则不再回源。
4. 监听反向表主库的binlog，当主库的binlog变化时，将缓存删掉，进行一个延迟删除，防止有旧值被别的请求更新到了缓存中
5. 本地缓存：
	1. 本地缓存，lru/lfu缓存，存n个关注/粉丝列表（命中率？）
	2. 热点缓存，对热点用户的关注/粉丝列表，关注关系定时拉取做本地缓存


#### 计数问题
1. 关注数：
关注数有上限，redis缓存里的value反序列化后的值会小于上线，直接使用缓存即可
2. 粉丝数：
粉丝数无上限，redis缓存里只存5000个，可能不准；5000个以上的粉丝提供离线服务来更新

#### 关注关系/粉丝关系判断
1. 关注关系：
	1. 如果是明星，走本地缓存
	2. 普通用户，走redis hash cache进行判断
		1. 如果关系判断参数to uid太多，为了防止redis性能降低，则不使用redis hash cache，而是拉取list cache，判断to uid是否在list中
	
2. 粉丝关系，没有上线，hash cache只存了关注关系，没有存粉丝关系，所以转化为正向关注关系进行判断
	1. 如果to uid比较小，则直接转换为正向关系判断（这里是多个key的关注hash ache，因此要使用redis pipeline）
	2. 如果to uid是明星，则走明星的本地关注列表缓存，看明星有没有关注from user id，即可判定明星是不是该from uid的粉丝
	3. 如果to uid的hash cache失效，则回源db，把该uid对应的关注者全部hmset一下

#### 拉列表
1. 关注列表：
	1. 明星，走local cache
	2. 正常用户，走list cache，如果miss则回源
2. 粉丝列表：
	1. 5000以内的直接走local cache、list cache即可，miss则回源
	2. 全量粉丝列表为粉丝数为几万～几千万的大v服务，可以考虑专门做一个离线服务，因为其特点如下：
		1. 粉丝数远超5000量级
		2. 访问5000以后的粉丝请求很少