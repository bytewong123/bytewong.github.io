---
title: 高并发系统设计40问——分布式服务篇
date: 2023-01-08T18:06:57+08:00
draft: false
categories: ["极客时间"]
tags: ["极客时间"]
---

# 高并发系统设计40问——分布式服务篇
#技术/极客时间/高并发系统设计40问

## 一体化架构的痛点
一体化架构的好处
- 开发简单直接，代码和项目集中式管理；
- 只需要维护一个工程，节省维护系统运行的人力成本；
- 排查问题的时候，只需要排查这个应用进程就可以了，目标性强。

一体化架构的缺陷
- 数据库连接数可能成为系统的瓶颈
系统是按照一体化架构部署的，在部署结构上没有分层，应用服务器直接连接数据库，那么当前端请求量增加，部署的应用服务器扩容，数据库的连接数也会大增
即使对数据库做了拆分，但是由于所有功能都在一个程序里，因此程序还是需要对每个库进行连接，并未减少数据库的连接数
- 一体化架构增加了研发的成本抑制了研发效率的提升
	- 代码冲突
	- 功能耦合，难以回归测试
	- 功能耦合，一个功能可能影响到别的功能稳定性
	- 运维困难，打包、部署慢，任何小的修改都需要上线整个项目
	- 难以扩容，只针对某一功能扩容会对整个功能都造成影响

## 微服务如何解决一体化的痛点
- 微服务拆分后，只有某一模块会连接其对应的数据库，其他模块则不会连接，这一降低了数据库的连接数，也更便于对该模块横向扩容；
- 每一个模块都实现相同的功能，不利于功能重用，将公用的功能下沉成服务，增加代码重用性
- 新的功能只需要测试自己的服务就可以了，而一旦服务出了问题，也可以通过服务熔断、降级的方式减少对于其他服务的影响
- 每个服务都只是原有系统的子集，代码行数相比原有系统要小很多，构建速度上也会有比较大的提升

## 微服务拆分原则
- 单一服务内部功能的高内聚和低耦合
每个服务只完成自己职责之内的任务，对于不是自己职责的功能交给其它服务来完成
> 判断用户是否为认证用户的逻辑应该内聚在用户服务内部，而不应该由内容服务判断，否则认证的逻辑一旦变更内容服务也需要一同跟着变更，这就不满足高内聚、低耦合的要求了。在 Review 代码时及时发现问题
- 需要关注服务拆分的粒度，先粗略拆分再逐渐细化
拆分初期可以把服务粒度拆得粗一些，后面随着团队对于业务和微服务理解的加深，再考虑把服务粒度细化
- 拆分的过程，要尽量避免影响产品的日常功能迭代
	- 优先剥离比较独立的边界服务（比如短信服务、地理位置服务），从非核心的服务出发减少拆分对现有业务的影响
	- 当两个服务存在依赖关系时优先拆分被依赖的服务
	- 微服务接口的定义要具备可扩展性，以免接口参数变化时导致接口报错

## 微服务化带来的问题
- 服务接口的调用不再是同一进程内的方法调用而是跨进程的网络调用，这会增加接口响应时间的增加
- 接口调用方需要知道服务部署在哪些机器的哪个端口上，这些信息需要存储在一个分布式一致性的存储中，于是就需要引入服务注册中心
- 多个服务之间有着错综复杂的依赖关系。一个服务会依赖多个其它服务也会被多个服务所依赖，那么一旦被依赖的服务的性能出现问题产生大量的慢请求，就会导致依赖服务的工作线程池中的线程被占满，依赖的服务也会出现性能问题
> 需要引入服务治理体系针对出问题的服务采用熔断、降级、限流、超时控制的方法

## rpc框架的关键技术
- 提升网络传输性能
选择一种高性能的 I/O 模型
等待资源：
	- 阻塞：在数据不可用时 I/O 请求一直阻塞，直到数据返回
	- 非阻塞：数据不可用时 I/O 请求立即返回，直到被通知资源可用为止
资源使用：从网络上接收到数据，并且拷贝到应用程序的缓冲区里面
	- 同步：I/O 请求在读取或者写入数据时会阻塞，直到读取或者写入数据完成；
	- 异步：I/O 请求在读取或者写入数据时立即返回，当操作系统处理完成 I/O 请求并且将数据拷贝到用户提供的缓冲区后，再通知应用 I/O 请求执行完成
以上两方面进行组合，可以有五种情况：
	- 同步阻塞 I/O；
	- 同步非阻塞 I/O；
	- 同步多路 I/O 复用；（非阻塞、同步）
	- 信号驱动 I/O；（非阻塞、同步）
	- 异步 I/O；
这五种 I/O 模型中最被广泛使用的是多路 I/O 复用

Nagle`s 算法希望：如果是连续的小数据包，大小没有一个 MSS（Maximum SegmentSize，最大分段大小），并且还没有收到之前发送的数据包的 Ack 信息，那么这些小数据包就会在发送端暂存起来，直到小数据包累积到一个 MSS，或者收到一个 Ack 为止，减少不必要的网络传输；

但是如果接收端开启了 DelayedACK（延迟 ACK 的发送，这样可以合并多个 ACK，提升网络传输效率），那就会发生发送端发送第一个数据包后接收端没有返回 ACK，这时发送端发送了第二个数据包，因为 Nagle`s 算法的存在，并且第一个发送包的 ACK 还没有返回，所以第二个包会暂存起来。而 DelayedACK 的超时时间默认是 40ms，所以一旦到了 40ms，接收端回给发送端 ACK，那么发送端才会发送第二个包，这样就增加了延迟；
只要在 Socket 上开启 tcp_nodelay 就好了，这个参数关闭了 Nagle`s 算法，这样发送端就不需要等到上一个发送包的 ACK 返回直接发送新的数据包就好了。这对于强网络交互的场景来说非常的适用，基本上，如果你要自己实现一套网络框架，tcp_nodelay 这个参数最好是要开启的


- 选择合适的序列化方式
一次 RPC 调用需要经历两次数据序列化的过程（请求体序列化，返回体序列化）和两次数据反序列化（反序列化请求体，反序列化返回体）的过程，可见它们对于 RPC 的性能影响是很大的

- 如果对于性能要求不高，在传输数据占用带宽不大的场景下可以使用 JSON 作为序列化协议；
- 如果对于性能要求比较高，那么使用 Thrift 或者 Protobuf 都可以。而 Thrift 提供了配套的 RPC 框架，所以想要一体化的解决方案，你可以优先考虑 Thrift；
- 在一些存储的场景下，比如说你的缓存中存储的数据占用空间较大，那么你可以考虑使用 Protobuf 替换 JSON 作为存储数据的序列化方式。


## 服务发现与注册中心
- 提供服务地址的存储
- 存储内容发生变更时，可以将变更的内容推送给客户端

### 服务状态管理
#### 心跳模式
- 注册中心为每一个连接上来的 RPC 服务节点记录最近续约的时间，RPC 服务节点在启动注册到注册中心后，就按照一定的时间间隔（比如 30 秒），向注册中心发送心跳包。
- 注册中心在接收到心跳包之后，会更新这个节点的最近续约时间。
- 注册中心会启动一个定时器定期检测当前时间和节点最近续约时间的差值，如果达到一个阈值（比如说 90 秒），那么认为这个服务节点不可用。
- 为了防止网络情况导致自动续期失败，从而使大量注册节点被下线，客户端整体访问不可用，需要给注册中心增加保护的策略：如果摘除的节点占到了服务集群节点数的 40%，就停止摘除服务节点，并且给服务的开发同学和运维同学报警处理
- 也可以给重要和一般的服务分别区分为永久节点和临时节点。永久节点只要不显式调用删除，那么会一直在注册中心中；临时节点则必须通过续期的方式保存；
#### 通知风暴
- 加入一些保护策略，比如说如果通知的消息量达到某一个阈值就停止变更通知
- 将全量数据变更改为增量数据变更
- 可以将事件消息合并推送
#### 数据一致性
可以参考raft的实现方式：[「图解Raft」让一致性算法变得更简单 - ZingLix Blog](https://zinglix.xyz/2020/06/25/raft/)

## 分布式trace
- traceId，区分一个请求的各种日志：将整个请求链路串起来，可以把日志发送到消息队列，然后发送到es
- spanId，记录服务之间的调用关系：A 服务在发起 RPC 请求服务 B 前，先从线程上下文中获取当前的 traceId 和 spanId，然后依据上面的逻辑生成本次 RPC 调用的 spanId，再将 spanId 和 traceId 序列化后装配到请求体中，发送给服务方 B

## 负载均衡
### 负载均衡类型
#### 代理类
- lvs
	- LVS 在 OSI 网络模型中的第四层，传输层工作，所以 LVS 又可以称为四层负载
	- LVS 是在网络栈的四层做请求包的转发，请求包转发之后，由客户端和后端服务直接建立连接，后续的响应包不会再经过 LVS 服务器，所以相比 Nginx 性能会更高，也能够承担更高的并发
	- LVS 缺陷是工作在四层，而请求的 URL 是七层的概念，不能针对 URL 做更细致的请求分发，而且 LVS 也没有提供探测后端服务是否存活的机制
	- LVS 适合在入口处承担大流量的请求分发
- nginx
	- Nginx 虽然比 LVS 的性能差很多，但也可以承担每秒几万次的请求，并且它在配置上更加灵活，还可以感知后端服务是否出现问题
	- Nginx 要部署在业务服务器之前做更细维度的请求分发

如果QPS 在十万以内，那么可以考虑不引入 LVS 而直接使用 Nginx 作为唯一的负载均衡服务器，这样少维护一个组件，也会减少系统的维护成本

这两个负载均衡服务适用于普通的 Web 服务，对于微服务架构来说，它们是不合适的。
因为微服务架构中的服务节点存储在注册中心里，使用 LVS 就很难和注册中心交互获取全量的服务节点列表。
另外，一般微服务架构中，使用的是 RPC 协议而不是 HTTP 协议，所以 Nginx 也不能满足要求

#### 客户端类
它一般和客户端应用部署在一个进程中，提供多种选择节点的策略，最终为客户端应用提供一个最佳的、可用的服务端节点。
这类服务一般会结合注册中心来使用，注册中心提供服务节点的完整列表，客户端拿到列表之后使用负载均衡服务的策略选取一个合适的节点，然后将请求发到这个节点上。

### 负载均衡策略
#### 静态策略
负载均衡服务器在选择服务节点时，不会参考后端服务的实际运行的状态；
- 轮询
- 带权重轮询
#### 动态策略
负载均衡服务器会依据后端服务的一些负载特性，来决定要选择哪一个服务节点
在负载均衡服务器上会收集对后端服务的调用信息，比如从负载均衡端到后端服务的活跃连接数，或者是调用的响应时间，然后从中选择连接数最少的服务，或者响应时间最短的后端服务

- LeastAcive 策略，就是优先选择活跃连接数最少的服务
- WeightedResponseTimeRule 是使用响应时间给每个服务节点计算一个权重，然后依据这个权重，来给调用方分配服务节点

这些策略的思考点是从调用方的角度出发，选择负载最小、资源最空闲的服务来调用，以期望能得到更高的服务调用性能，也就能最大化地使用服务器的空闲资源，请求也会响应得更迅速。所以在实际开发中，优先考虑使用动态的策略。

### 故障检测
淘宝开源的 Nginx 模块nginx_upstream_check_module了，这个模块可以让 Nginx 定期地探测后端服务的一个指定的接口，然后根据返回的状态码来判断服务是否还存活。当探测不存活的次数达到一定阈值时，就自动将这个后端服务从负载均衡服务器中摘除

在服务刚刚启动时，可以初始化默认的 HTTP 状态码是 500，这样 Nginx 就不会很快将这个服务节点标记为可用，也就可以等待服务中依赖的资源初始化完成，避免服务初始启动时的波动。
在完全初始化之后，再将 HTTP 状态码变更为 200，Nginx 经过两次探测后，就会标记服务为可用。
在服务关闭时，也应该先将 HTTP 状态码变更为 500，等待 Nginx 探测将服务标记为不可用后，前端的流量也就不会继续发往这个服务节点。
在等待服务正在处理的请求全部处理完毕之后，再对服务做重启，可以避免直接重启导致正在处理的请求失败的问题。这是启动和关闭线上 Web 服务时的标准姿势，你可以在项目中参考使用。

### API网关
API 网关分为入口网关和出口网关两类
- 入口网关作用很多，可以隔离客户端和微服务，从中提供协议转换、安全策略、认证、限流、熔断等功能。
- 出口网关主要是为调用第三方服务提供统一的出口，在其中可以对调用外部的 API 做统一的认证、授权、审计以及访问控制；

### 多机房部署
多机房部署的含义是：在不同的 IDC 机房中部署多套服务，这些服务共享同一份业务数据，并且都可以承接来自用户的流量。

这样，当其中某一个机房出现网络故障、火灾，甚至整个城市发生地震、洪水等大的不可抗的灾难时，你可以随时将用户的流量切换到其它地域的机房中，从而保证系统可以不间断地持续运行。

#### 逐步迭代的多机房部署方案
##### 同城双活
同城机房之间的延时在 1ms~3ms 左右，对于跨机房调用的容忍度比较高，所以，这种同城双活的方案复杂度会比较低。
但是，它只能做到机房级别的容灾，无法做到城市级别的容灾。不过，相比于城市发生地震、洪水等自然灾害来说，机房网络故障、掉电出现的概率要大得多。所以，如果你的系统不需要考虑城市级别的容灾，一般做到同城双活就足够了。
在北京有 A 和 B 两个机房，A 是联通的机房，B 是电信的机房，机房之间以专线连接，方案设计时，核心思想是尽量避免跨机房的调用
	- 数据库的主库可以部署在一个机房中，比如部署在 A 机房中，那么 A 和 B 机房数据都会被写入到 A 机房中。
	- 在 A、B 两个机房中各部署一个从库，通过主从复制的方式，从主库中同步数据，这样双机房的查询请求可以查询本机房的从库
	- 一旦 A 机房发生故障，可以通过主从切换的方式将 B 机房的从库提升为主库，达到容灾的目的。
	- 缓存也可以部署在两个机房中，查询请求也读取本机房的缓存，如果缓存中数据不存在，就穿透到本机房的从库中加载数据。数据的更新可以更新双机房中的数据，保证数据的一致性。
	- 不同机房的 RPC 服务会向注册中心注册不同的服务组，而不同机房的 RPC 客户端，只订阅同机房的 RPC 服务组，这样就可以实现 RPC 调用尽量发生在本机房内，避免跨机房的 RPC 调用。
	- 你的系统肯定会依赖公司内的其他服务，比如审核、搜索等服务，如果这些服务也是双机房部署的，那么也需要尽量保证只调用本机房的服务，降低调用的延迟。
	- 使用了同城双活架构之后，可以实现机房级别的容灾，服务的部署也能够突破单一机房的限制。但是，还是会存在跨机房写数据的问题，不过由于写数据的请求量不高，所以在性能上是可以容忍的。
##### 异地多活
参考文章[搞懂异地多活，看这篇就够了 | Kaito’s Blog](http://kaito-kidd.com/2021/10/15/what-is-the-multi-site-high-availability-design/)

要考虑即使机房所在的城市发生重大的自然灾害，也要保证系统的可用性。而这时，你需要采用异地多活的方案。

在考虑异地多活方案时，你首先要考虑异地机房的部署位置。
它部署的不能太近，否则发生自然灾害时，很可能会波及。所以，如果你的主机房在北京，那么异地机房就尽量不要建设在天津，而是可以选择上海、广州这样距离较远的位置。
但这就会造成更高的数据传输延迟，同城双活中，使用的跨机房写数据库的方案，就不合适了。

所以，在数据写入时，你要保证只写本机房的数据存储服务再采取数据同步的方案，将数据同步到异地机房中。一般来说，数据同步的方案有两种：
- 一种基于存储系统的主从复制，比如 MySQL 和 Redis。也就是在一个机房部署主库，在异地机房部署从库，两者同步主从复制实现数据的同步。这种架构是一种双主架构，mysql提供了这种架构（基于自主主键控制，通过设置自增属性auto_increment_offset和auto_increment_increment来控制每个主节点生产不同的自增值），但是redis、mongodb没有，需要开发中间件实现
	- 冲突处理：两个机房都可以写，操作的不是同一条数据那还好，如果修改的是同一条的数据，发生冲突怎么办？
> 1. 用户短时间内发了 2 个修改请求，都是修改同一条数据
> 2. 一个请求落在北京机房，修改 X = 1（还未同步到上海机房）
> 3. 另一个请求落在上海机房，修改 X = 2（还未同步到北京机房）
> 4. 两个机房以哪个为准？
从「源头」就避免数据冲突的发生，在最上层就把用户「区分」开，部分用户请求固定打到北京机房，其它用户请求固定打到上海 机房，进入某个机房的用户请求，之后的所有业务操作，都在这一个机房内完成，从根源上避免「跨机房」。
需要在接入层之上，再部署一个「路由层」（通常部署在云服务器上），自己可以配置路由规则，把用户「分流」到不同的机房内。
但这个路由规则，具体怎么定呢？常见的有3 类：
1. 按业务类型分片
2. uid哈希分片
3. 按地理位置分片

分片的核心思路在于，**让同一个用户的相关请求，只在一个机房内完成所有业务「闭环」，不再出现「跨机房」访问**。

两个机房就可以都接收「读写」流量（做好分片的请求），底层存储保持「双向」同步，两个机房都拥有全量数据，当任意机房故障时，另一个机房就可以「接管」全部流量，实现快速切换

这里还有一种情况，是无法做数据分片的：**全局数据**。例如系统配置、商品库存这类需要强一致的数据，这类服务依旧只能采用写主机房，读从机房的方案，不做双活。
双活的重点，是要优先保证「核心」业务先实现双活，并不是「全部」业务实现双活。

## service mesh
使用新语言开发的微服务，无法使用之前积累的服务治理的策略。
比如说，RPC 客户端在使用注册中心订阅服务的时候，为了避免每次 RPC 调用都要与注册中心交互，一般会在 RPC 客户端缓存节点的数据。如果注册中心中的服务节点发生了变更，那么 RPC 客户端的节点缓存会得到通知，并且变更缓存数据。
而且，为了减少注册中心的访问压力，我们在 RPC 客户端上一般会考虑使用多级缓存（内存缓存和文件缓存）来保证节点缓存的可用性。
而这些策略在开始的时候，都是使用 Java 语言来实现的，并且封装在注册中心客户端里提供给 RPC 客户端使用。
如果更换了新的语言，这些逻辑就都要使用新的语言实现一套。
除此之外，负载均衡、熔断降级、流量控制、打印分布式追踪日志等等，这些服务治理的策略都需要重新实现，而使用其它语言重新实现这些策略无疑会带来巨大的工作量，也是中间件研发中一个很大的痛点。

可以考虑将服务治理的细节，从 RPC 客户端中拆分出来，形成一个代理层单独部署。这个代理层可以使用单一的语言实现，所有的流量都经过代理层来使用其中的服务治理策略。这是一种“关注点分离”的实现方式，也是 Service Mesh 的核心思想。

### 什么是service mesh
Service Mesh 主要处理服务之间的通信，它的主要实现形式就是在应用程序同主机上部署一个代理程序。一般来讲，我们将这个代理程序称为“Sidecar（边车）”，服务之间的通信也从之前的客户端和服务端直连，变成了客户端与mesh通信，mesh再与服务端通信

#### 如何将流量转发到sidecar
##### iptables
使用 iptables 的方式来实现流量透明的转发，而 Istio 就默认了使用 iptables 来实现数据包的转发。

Iptables 是 Linux 内核中，防火墙软件 Netfilter 的管理工具，它位于用户空间，可以控制 Netfilter 实现地址转换的功能。在 iptables 中默认有五条链，你可以把这五条链当作数据包流转过程中的五个步骤，依次为
- PREROUTING
- INPUT
- FORWARD
- OUTPUT
- POSTROUTING
[image:2D387FA6-A12F-4048-ABA1-4939E5764C72-411-000280B93275857D/E8AE3304-12BB-4091-90B2-F8505C699829.png]

##### sdk
通过sdk，将请求直接发送给sidecar，sidecar再将请求转发




